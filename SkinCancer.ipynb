{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Skin Cancer Classification and Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 2. Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 7011\n",
      "Validation set: 1502\n",
      "Test set: 1503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking corrupt files:  47%|████▋     | 3330/7011 [00:06<00:05, 619.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupt or missing image archive/images\\image.jpg: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Mohamed Sakr\\\\Downloads\\\\PatternRecognition_Assignment2\\\\archive\\\\images\\\\image.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking corrupt files: 100%|██████████| 7011/7011 [00:11<00:00, 596.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size after image check: 7010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking corrupt files: 100%|██████████| 1502/1502 [00:02<00:00, 661.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set size after image check: 1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking corrupt files: 100%|██████████| 1503/1503 [00:01<00:00, 776.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size after image check: 1503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking corrupt files: 100%|██████████| 7010/7010 [00:11<00:00, 610.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Train set size after mask check: 7010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking corrupt files: 100%|██████████| 1502/1502 [00:04<00:00, 317.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Val set size after mask check: 1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking corrupt files: 100%|██████████| 1503/1503 [00:04<00:00, 333.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Test set size after mask check: 1503\n",
      "\n",
      "Class distributions:\n",
      "Training: dx\n",
      "nevus                   0.669472\n",
      "melanoma                0.111127\n",
      "benign_keratosis        0.109700\n",
      "basal_cell_carcinoma    0.051355\n",
      "actinic_keratosis       0.032668\n",
      "vascular_lesion         0.014123\n",
      "dermatofibroma          0.011555\n",
      "Name: proportion, dtype: float64\n",
      "Validation: dx\n",
      "nevus                   0.669774\n",
      "melanoma                0.111185\n",
      "benign_keratosis        0.109854\n",
      "basal_cell_carcinoma    0.051265\n",
      "actinic_keratosis       0.032623\n",
      "vascular_lesion         0.013981\n",
      "dermatofibroma          0.011318\n",
      "Name: proportion, dtype: float64\n",
      "Test: dx\n",
      "nevus                   0.669328\n",
      "melanoma                0.111111\n",
      "benign_keratosis        0.109780\n",
      "basal_cell_carcinoma    0.051231\n",
      "actinic_keratosis       0.032601\n",
      "vascular_lesion         0.014637\n",
      "dermatofibroma          0.011311\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Label mapping: {'actinic_keratosis': 0, 'basal_cell_carcinoma': 1, 'benign_keratosis': 2, 'dermatofibroma': 3, 'melanoma': 4, 'nevus': 5, 'vascular_lesion': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load metadata with column names\n",
    "column_names = ['image_id'] + [f'class_{i}' for i in range(7)]\n",
    "metadata = pd.read_csv('archive/GroundTruth.csv', header=None, names=column_names)\n",
    "\n",
    "# Define class names\n",
    "class_names = [\n",
    "    'melanoma',\n",
    "    'nevus',\n",
    "    'basal_cell_carcinoma',\n",
    "    'actinic_keratosis',\n",
    "    'benign_keratosis',\n",
    "    'dermatofibroma',\n",
    "    'vascular_lesion'\n",
    "]\n",
    "\n",
    "# Convert one-hot to dx column\n",
    "metadata['dx'] = metadata.iloc[:, 1:8].idxmax(axis=1)\n",
    "metadata['dx'] = metadata['dx'].str.replace('class_', '').map(lambda x: class_names[int(x)])\n",
    "\n",
    "# Check for duplicates\n",
    "if metadata.duplicated().any():\n",
    "    print(\"Warning: Duplicates found. Removing duplicates.\")\n",
    "    metadata = metadata.drop_duplicates()\n",
    "\n",
    "# Split data\n",
    "try:\n",
    "    train_df, temp_df = train_test_split(metadata, test_size=0.3, stratify=metadata['dx'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['dx'])\n",
    "except ValueError as e:\n",
    "    print(f\"Splitting error: {e}\")\n",
    "    print(\"Consider merging rare classes or using a different strategy.\")\n",
    "    raise\n",
    "\n",
    "# Reset indices\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print stats\n",
    "print(f\"\\nTraining set: {len(train_df)}\")\n",
    "print(f\"Validation set: {len(val_df)}\")\n",
    "print(f\"Test set: {len(test_df)}\")\n",
    "\n",
    "# Check corrupt samples with optional mask requirement\n",
    "def check_corrupt_samples(df, image_dir, mask_dir=None, require_masks=False):\n",
    "    corrupt_indices = []\n",
    "    for idx in tqdm(range(len(df)), desc=\"Checking corrupt files\"):\n",
    "        img_name = df.loc[idx, 'image_id'] + '.jpg'\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        \n",
    "        # Check image\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()\n",
    "        except (IOError, OSError, ValueError) as e:\n",
    "            print(f\"Corrupt or missing image {img_path}: {e}\")\n",
    "            corrupt_indices.append(idx)\n",
    "            continue\n",
    "        \n",
    "        # Check mask only if required\n",
    "        if require_masks and mask_dir:\n",
    "            mask_name = df.loc[idx, 'image_id'] + '_segmentation.png'  # Updated to match actual naming\n",
    "            mask_path = os.path.join(mask_dir, mask_name)\n",
    "            if os.path.exists(mask_path):\n",
    "                try:\n",
    "                    with Image.open(mask_path) as mask:\n",
    "                        mask.verify()\n",
    "                except (IOError, OSError, ValueError) as e:\n",
    "                    print(f\"Corrupt or missing mask {mask_path}: {e}\")\n",
    "                    corrupt_indices.append(idx)\n",
    "            else:\n",
    "                print(f\"Missing mask: {mask_name}\")\n",
    "                corrupt_indices.append(idx)\n",
    "    \n",
    "    return corrupt_indices\n",
    "\n",
    "# Process datasets for classification (images only)\n",
    "for df, name in [(train_df, \"train\"), (val_df, \"val\"), (test_df, \"test\")]:\n",
    "    corrupt = check_corrupt_samples(df, 'archive/images', mask_dir=None, require_masks=False)\n",
    "    if corrupt:\n",
    "        df.drop(corrupt, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"{name.capitalize()} set size after image check: {len(df)}\")\n",
    "\n",
    "# Create a separate copy for segmentation (images + masks)\n",
    "seg_train_df = train_df.copy()\n",
    "seg_val_df = val_df.copy()\n",
    "seg_test_df = test_df.copy()\n",
    "\n",
    "for df, name in [(seg_train_df, \"train\"), (seg_val_df, \"val\"), (seg_test_df, \"test\")]:\n",
    "    corrupt = check_corrupt_samples(df, 'archive/images', 'archive/masks', require_masks=True)\n",
    "    if corrupt:\n",
    "        df.drop(corrupt, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Segmentation {name.capitalize()} set size after mask check: {len(df)}\")\n",
    "    if len(df) == 0:\n",
    "        print(f\"Warning: Segmentation {name} set is empty. Masks may be missing in 'archive/masks'.\")\n",
    "\n",
    "# Create label mapping\n",
    "all_data = pd.concat([train_df, val_df, test_df])\n",
    "label_mapping = {label: idx for idx, label in enumerate(sorted(all_data['dx'].unique()))}\n",
    "\n",
    "print(\"\\nClass distributions:\")\n",
    "print(\"Training:\", train_df['dx'].value_counts(normalize=True))\n",
    "print(\"Validation:\", val_df['dx'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['dx'].value_counts(normalize=True))\n",
    "print(\"\\nLabel mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 3. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Classes\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_id'] + '.jpg'\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.label_mapping[self.df.iloc[idx]['dx']]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_id'] + '.jpg'\n",
    "        mask_name = self.df.iloc[idx]['image_id'] + '_segmentation.png'  # Updated to match actual naming\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "            \n",
    "        return image, mask\n",
    "\n",
    "# Transforms\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=InterpolationMode.NEAREST),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "classification_train = ClassificationDataset(train_df, 'archive/images', img_transform)\n",
    "classification_val = ClassificationDataset(val_df, 'archive/images', img_transform)\n",
    "classification_test = ClassificationDataset(test_df, 'archive/images', img_transform)\n",
    "\n",
    "# Create segmentation datasets\n",
    "segmentation_train = SegmentationDataset(seg_train_df, 'archive/images', 'archive/masks', \n",
    "                                       img_transform, mask_transform)\n",
    "segmentation_val = SegmentationDataset(seg_val_df, 'archive/images', 'archive/masks',\n",
    "                                     img_transform, mask_transform)\n",
    "segmentation_test = SegmentationDataset(seg_test_df, 'archive/images', 'archive/masks',\n",
    "                                      img_transform, mask_transform)\n",
    "\n",
    "# Check for empty datasets\n",
    "for dataset, name in [(classification_train, \"classification_train\"),\n",
    "                      (classification_val, \"classification_val\"),\n",
    "                      (segmentation_train, \"segmentation_train\"),\n",
    "                      (segmentation_val, \"segmentation_val\")]:\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(f\"Dataset {name} is empty after preprocessing. Check file paths or data availability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 4. Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed Sakr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed Sakr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model parameters: 11,180,103\n"
     ]
    }
   ],
   "source": [
    "class SkinCancerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super().__init__()\n",
    "        # Load pre-trained ResNet-18\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Replace final fully connected layer\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Instantiate and verify model\n",
    "classification_model = SkinCancerClassifier().to(device)\n",
    "print(f\"Classification model parameters: {sum(p.numel() for p in classification_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 5. Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation model parameters: 13,294,697\n"
     ]
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "        # Encoder (ResNet-18 backbone)\n",
    "        self.encoder = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            self.encoder.conv1,\n",
    "            self.encoder.bn1,\n",
    "            self.encoder.relu,\n",
    "            self.encoder.maxpool\n",
    "        )\n",
    "        self.encoder1 = self.encoder.layer1  # Output: 64 channels\n",
    "        self.encoder2 = self.encoder.layer2  # Output: 128 channels\n",
    "        self.encoder3 = self.encoder.layer3  # Output: 256 channels\n",
    "        self.encoder4 = self.encoder.layer4  # Output: 512 channels\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder4 = DecoderBlock(512, 256)  # Upsample to ~14x14\n",
    "        self.decoder3 = DecoderBlock(256, 128)  # Upsample to ~28x28\n",
    "        self.decoder2 = DecoderBlock(128, 64)   # Upsample to ~56x56\n",
    "        self.decoder1 = DecoderBlock(64, 64)    # Upsample to ~112x112\n",
    "        \n",
    "        # Additional upsampling to reach 224x224\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.conv1(x)       # Initial conv + maxpool\n",
    "        e1 = self.encoder1(x)   # Layer 1\n",
    "        e2 = self.encoder2(e1)  # Layer 2\n",
    "        e3 = self.encoder3(e2)  # Layer 3\n",
    "        e4 = self.encoder4(e3)  # Layer 4\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d4 = self.decoder4(e4) + e3  # Upsample + skip connection\n",
    "        d3 = self.decoder3(d4) + e2  # Upsample + skip connection\n",
    "        d2 = self.decoder2(d3) + e1  # Upsample + skip connection\n",
    "        d1 = self.decoder1(d2)       # Upsample to ~112x112\n",
    "        \n",
    "        # Additional upsampling to 224x224\n",
    "        d1 = self.upsample(d1)\n",
    "        \n",
    "        # Final output\n",
    "        return self.final(d1)\n",
    "\n",
    "# Instantiate and verify model\n",
    "segmentation_model = UNet().to(device)\n",
    "print(f\"Segmentation model parameters: {sum(p.numel() for p in segmentation_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 6. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, task='classification'):\n",
    "   \n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for inputs, targets in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if task == 'segmentation':\n",
    "                loss = criterion(outputs, targets)\n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, task)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        if task == 'classification':\n",
    "            history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'best_{task}_model.pth')\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"{task.capitalize()} Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}\" + \n",
    "              (f\", Val Acc: {val_acc:.2f}%\" if task == 'classification' else \"\"))\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, task='classification'):\n",
    "    \n",
    "  \n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if task == 'segmentation':\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    if task == 'classification':\n",
    "        accuracy = 100 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "    return avg_loss, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 220/220 [04:33<00:00,  1.24s/it, loss=0.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 1: Train Loss: 0.6947, Val Loss: 0.5116, Val Acc: 80.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 220/220 [04:19<00:00,  1.18s/it, loss=0.268] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 2: Train Loss: 0.2959, Val Loss: 0.5018, Val Acc: 81.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 220/220 [04:30<00:00,  1.23s/it, loss=3.62]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 3: Train Loss: 0.1454, Val Loss: 0.5766, Val Acc: 82.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 220/220 [04:24<00:00,  1.20s/it, loss=3.03]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 4: Train Loss: 0.1124, Val Loss: 0.6730, Val Acc: 82.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 220/220 [04:09<00:00,  1.13s/it, loss=0.654]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 5: Train Loss: 0.0730, Val Loss: 0.7626, Val Acc: 81.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 220/220 [04:02<00:00,  1.10s/it, loss=0.669]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 6: Train Loss: 0.0715, Val Loss: 0.5486, Val Acc: 85.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 220/220 [04:04<00:00,  1.11s/it, loss=0.0142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 7: Train Loss: 0.0323, Val Loss: 0.5985, Val Acc: 84.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 220/220 [04:03<00:00,  1.11s/it, loss=0.0138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 8: Train Loss: 0.0170, Val Loss: 0.5640, Val Acc: 85.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 220/220 [03:59<00:00,  1.09s/it, loss=0.215]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 9: Train Loss: 0.0080, Val Loss: 0.5921, Val Acc: 85.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 220/220 [04:00<00:00,  1.09s/it, loss=5.9]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 10: Train Loss: 0.0581, Val Loss: 0.7054, Val Acc: 84.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 220/220 [04:07<00:00,  1.12s/it, loss=0.388]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 11: Train Loss: 0.0350, Val Loss: 0.7984, Val Acc: 82.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 220/220 [03:59<00:00,  1.09s/it, loss=2.67]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 12: Train Loss: 0.0831, Val Loss: 0.6101, Val Acc: 84.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 220/220 [04:00<00:00,  1.09s/it, loss=1.12]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 13: Train Loss: 0.0528, Val Loss: 0.6714, Val Acc: 85.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 220/220 [04:21<00:00,  1.19s/it, loss=0.009]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 14: Train Loss: 0.0462, Val Loss: 0.6067, Val Acc: 85.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 220/220 [04:06<00:00,  1.12s/it, loss=0.084]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Epoch 15: Train Loss: 0.0164, Val Loss: 0.6530, Val Acc: 84.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 439/439 [07:38<00:00,  1.05s/it, loss=0.383] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Epoch 1: Train Loss: 0.2056, Val Loss: 0.1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 439/439 [07:47<00:00,  1.06s/it, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Epoch 2: Train Loss: 0.1042, Val Loss: 0.0991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 439/439 [11:27<00:00,  1.57s/it, loss=0.0449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Epoch 3: Train Loss: 0.0799, Val Loss: 0.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 439/439 [08:28<00:00,  1.16s/it, loss=0.0311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Epoch 4: Train Loss: 0.0665, Val Loss: 0.0928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 439/439 [07:37<00:00,  1.04s/it, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Epoch 5: Train Loss: 0.0598, Val Loss: 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25:  13%|█▎        | 56/439 [2:04:35<07:55,  1.24s/it, loss=0.0948]      "
     ]
    }
   ],
   "source": [
    "# Train Classification Model\n",
    "clf_criterion = nn.CrossEntropyLoss()\n",
    "clf_optimizer = optim.Adam(classification_model.parameters(), lr=1e-4)\n",
    "clf_history = train_model(classification_model, \n",
    "                         DataLoader(classification_train, 32, shuffle=True),\n",
    "                         DataLoader(classification_val, 32),\n",
    "                         clf_criterion, clf_optimizer, \n",
    "                         num_epochs=15, task='classification')\n",
    "\n",
    "# Train Segmentation Model\n",
    "seg_criterion = nn.BCEWithLogitsLoss()\n",
    "seg_optimizer = optim.Adam(segmentation_model.parameters(), lr=1e-4)\n",
    "seg_history = train_model(segmentation_model, \n",
    "                         DataLoader(segmentation_train, 16, shuffle=True),\n",
    "                         DataLoader(segmentation_val, 16),\n",
    "                         seg_criterion, seg_optimizer,\n",
    "                         num_epochs=25, task='segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 8. Evaluation and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_dice(pred_mask, true_mask):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) and Dice coefficient.\"\"\"\n",
    "    pred_mask = (pred_mask > 0.5).float()\n",
    "    true_mask = (true_mask > 0.5).float()\n",
    "    \n",
    "    intersection = (pred_mask * true_mask).sum()\n",
    "    union = pred_mask.sum() + true_mask.sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    dice = (2 * intersection + 1e-6) / (pred_mask.sum() + true_mask.sum() + 1e-6)\n",
    "    return iou.item(), dice.item()\n",
    "\n",
    "def generate_classification_report(model, loader):\n",
    "    \"\"\"Generate classification report with metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_targets, all_preds, target_names=label_mapping.keys()))\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(confusion_matrix(all_targets, all_preds), \n",
    "                annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_mapping.keys(),\n",
    "                yticklabels=label_mapping.keys())\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def generate_segmentation_report(model, loader):\n",
    "    \"\"\"Generate segmentation metrics and visualizations.\"\"\"\n",
    "    model.eval()\n",
    "    ious, dices = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "            preds = (outputs > 0.5).float()\n",
    "            \n",
    "            for pred, mask in zip(preds, masks.to(device)):\n",
    "                iou, dice = calculate_iou_dice(pred, mask)\n",
    "                ious.append(iou)\n",
    "                dices.append(dice)\n",
    "    \n",
    "    print(f\"Mean IoU: {np.mean(ious):.4f}\")\n",
    "    print(f\"Mean Dice: {np.mean(dices):.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    sample = next(iter(loader))\n",
    "    inputs, masks = sample\n",
    "    outputs = torch.sigmoid(model(inputs.to(device)))\n",
    "    preds = (outputs > 0.5).float().cpu()\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 4, i * 4 + 1)\n",
    "        plt.imshow(inputs[i].permute(1, 2, 0).numpy() * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, i * 4 + 2)\n",
    "        plt.imshow(masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, i * 4 + 3)\n",
    "        plt.imshow(preds[i].squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, i * 4 + 4)\n",
    "        plt.imshow((preds[i].squeeze() > 0.5).astype(float), cmap='jet', alpha=0.5)\n",
    "        plt.imshow(inputs[i].permute(1, 2, 0).numpy() * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])\n",
    "        plt.title('Overlay')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate final reports\n",
    "print(\"\\nFinal Classification Performance:\")\n",
    "generate_classification_report(classification_model,  DataLoader(classification_test, 32))\n",
    "\n",
    "print(\"\\nFinal Segmentation Performance:\")\n",
    "generate_segmentation_report(segmentation_model,\n",
    "                            DataLoader(segmentation_test, 16))\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(clf_history['train_loss'], label='Train Loss')\n",
    "plt.plot(clf_history['val_loss'], label='Val Loss')\n",
    "plt.plot(clf_history['val_acc'], label='Val Accuracy')\n",
    "plt.title('Classification Training')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(seg_history['train_loss'], label='Train Loss')\n",
    "plt.plot(seg_history['val_loss'], label='Val Loss')\n",
    "plt.title('Segmentation Training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 9. Single Model for Both Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super().__init__()\n",
    "        # Shared encoder\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.base_model.conv1,\n",
    "            self.base_model.bn1,\n",
    "            self.base_model.relu,\n",
    "            self.base_model.maxpool,\n",
    "            self.base_model.layer1,\n",
    "            self.base_model.layer2,\n",
    "            self.base_model.layer3,\n",
    "            self.base_model.layer4\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Segmentation decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            DecoderBlock(512, 256),\n",
    "            DecoderBlock(256, 128),\n",
    "            DecoderBlock(128, 64),\n",
    "            DecoderBlock(64, 64),\n",
    "            nn.Conv2d(64, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        cls_out = self.classifier(features)\n",
    "        seg_out = self.decoder(features)\n",
    "        return cls_out, seg_out\n",
    "\n",
    "# Initialize and verify\n",
    "multi_task_model = MultiTaskModel().to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in multi_task_model.parameters()):,}\")\n",
    "\n",
    "# Create multi-task dataset\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, mask_dir):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize(224, InterpolationMode.NEAREST),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, f\"{self.df.iloc[idx]['image_id']}.jpg\")\n",
    "        mask_path = os.path.join(self.mask_dir, f\"{self.df.iloc[idx]['image_id']}_mask.jpg\")\n",
    "        label = label_mapping[self.df.iloc[idx]['dx']]\n",
    "        return self.transform(Image.open(img_path)), (\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "            self.mask_transform(Image.open(mask_path).convert('L'))\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "# Training loop\n",
    "multitask_train = MultiTaskDataset(train_df, 'archive/images', 'archive/masks')\n",
    "train_loader = DataLoader(multitask_train, 16, shuffle=True)\n",
    "\n",
    "clf_criterion = nn.CrossEntropyLoss()\n",
    "seg_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(multi_task_model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    multi_task_model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, (labels, masks) in tqdm(train_loader):\n",
    "        images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cls_pred, seg_pred = multi_task_model(images)\n",
    "        loss = clf_criterion(cls_pred, labels) + seg_criterion(seg_pred, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "test_loader = DataLoader(\n",
    "    MultiTaskDataset(test_df, 'archive/images', 'archive/masks'), \n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_acc, total_iou, total_dice = 0, 0, 0\n",
    "    for images, (labels, masks) in test_loader:\n",
    "        images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "        cls_pred, seg_pred = multi_task_model(images)\n",
    "        \n",
    "        total_acc += (cls_pred.argmax(1) == labels).float().mean().item()\n",
    "        iou, dice = calculate_iou_dice(seg_pred, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "\n",
    "    print(f\"Accuracy: {total_acc/len(test_loader):.4f}, \"\n",
    "          f\"IoU: {total_iou/len(test_loader):.4f}, \"\n",
    "          f\"Dice: {total_dice/len(test_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
